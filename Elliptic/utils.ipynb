{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T11:41:11.065486Z",
     "start_time": "2023-05-28T11:41:09.009571900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import pandas as pd\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from pygsp import graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T13:27:11.124672300Z",
     "start_time": "2023-05-28T13:27:11.109020200Z"
    }
   },
   "outputs": [],
   "source": [
    "def time_step_split_helper(new_nodes, new_edges, labels):\n",
    "    \"\"\"\n",
    "    Split the graph and store node features, edges (represented by adjacency list),\n",
    "    and labels separately by timestamp t (from 1 to 49).\n",
    "\n",
    "    Args:\n",
    "        new_nodes     A dataframe of the node features\n",
    "        new_edges     A dataframe of the graph's adjacency list\n",
    "\n",
    "    Returns:\n",
    "        features_t    A list of (|N_t|, d) feature matrices by timestamp\n",
    "        edge_indices  A list of (2, |E_t|) adjacency list by timestamp\n",
    "        labels_t      A list of (|N_t|) labels by timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    features =  torch.FloatTensor(new_nodes.iloc[:, 2:].to_numpy())\n",
    "    times = new_nodes.iloc[:, 1].to_numpy()\n",
    "    times = torch.LongTensor(times.reshape(len(times),))\n",
    "    labels = labels.iloc[:, 1].to_numpy().astype(int)\n",
    "    labels = torch.LongTensor(labels.reshape(len(labels),))\n",
    "    labels -= 1\n",
    "\n",
    "    nodes_id = new_nodes.iloc[:, 0].to_numpy()\n",
    "    nodes_id = torch.LongTensor(nodes_id.reshape(len(nodes_id),))\n",
    "\n",
    "    min_t = torch.min(times) # 1\n",
    "    max_t = torch.max(times) # 49\n",
    "\n",
    "    # Construct nodes of the directed graph for each time step;\n",
    "    # features by time step are stored in \"features_t\"; labels by\n",
    "    # time step are stored in \"labels_t\"\n",
    "    features_t = []\n",
    "    labels_t = []\n",
    "\n",
    "    # Create a dictionary where\n",
    "    # <key, value> = <node_id, <<idx, node_index_in_time_t_subgraph>, <t, time_t>>>.\n",
    "    id2idx = {}\n",
    "    for t in range(min_t, max_t + 1):\n",
    "        features_t.append(features[times == t, :])\n",
    "        labels_t.append(labels[times == t])\n",
    "        nodes_t = nodes_id[times == t]\n",
    "        for i in range(nodes_t.shape[0]):\n",
    "            id2idx[nodes_t[i].item()] = {}\n",
    "            id2idx[nodes_t[i].item()]['idx'] = i\n",
    "            id2idx[nodes_t[i].item()]['t'] = t\n",
    "\n",
    "    # Construct adjacency lists of the directed graph (non-symmetric) for each time step;\n",
    "    # adjacency lists for each time step are stored in \"edge_indices\".\n",
    "    edge_idx_t = [[] for _ in range(min_t, max_t + 1)]\n",
    "    for index in range(new_edges.shape[0]):\n",
    "        node1_t = id2idx[new_edges.iloc[index, 0]]['t']\n",
    "        node1_idx = id2idx[new_edges.iloc[index, 0]]['idx']\n",
    "        node2_t = id2idx[new_edges.iloc[index, 1]]['t']\n",
    "        node2_idx = id2idx[new_edges.iloc[index, 1]]['idx']\n",
    "        edge_idx_t[node1_t - 1].append([node1_idx, node2_idx]) # time_step starts from 1\n",
    "\n",
    "    edge_indices = [torch.LongTensor(edge_idx_t[i]).t() for i in range(len(edge_idx_t))]\n",
    "    return features_t, edge_indices, labels_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T11:41:11.112241800Z",
     "start_time": "2023-05-28T11:41:11.096620100Z"
    }
   },
   "outputs": [],
   "source": [
    "def time_step_split(new_nodes, new_edges, labels, device, train_lt = 31, val_lt = 36, test_lt = 49):\n",
    "    \"\"\"\n",
    "    Create and return the training, validation, and test set, splitted by time step,\n",
    "    where each subgraph at time t is considered as an input of GCN model.\n",
    "\n",
    "    Args:\n",
    "        new_nodes     A dataframe of the node features\n",
    "        new_edges     A dataframe of the graph's adjacency list\n",
    "        device        Computing device\n",
    "        train_lt      The last time step index of training set\n",
    "        val_lt        The last time step index of validation set\n",
    "        test_lt       The last time step index of test set\n",
    "\n",
    "    Returns:\n",
    "        data          A dictionary that stores training, validation, and test set,\n",
    "                        each value is a list of Data object\n",
    "        graph_info    A matrix where each row contains information of the time-step subgraph\n",
    "                      [time_step, num_of_nodes, num_of_edges, num_of_illicit_nodes]\n",
    "    \"\"\"\n",
    "    features_t, edge_indices, labels_t = time_step_split_helper(new_nodes, new_edges, labels)\n",
    "\n",
    "    graph_info = np.zeros((len(labels_t), 4), dtype = np.int64)\n",
    "    # for t in range(len(labels_t)):\n",
    "    #     if(edge_indices[t].shape != 0):\n",
    "    #         break\n",
    "    #     graph_info[t, :] = np.array([t, features_t[t].shape[0], edge_indices[t].shape[1],\n",
    "    #                                  labels_t[t][labels_t[t] == 1].shape[0]])\n",
    "\n",
    "    train_idx, val_idx, test_idx = [np.arange(train_lt), np.arange(train_lt, val_lt),\n",
    "                                    np.arange(val_lt, test_lt)]\n",
    "    train_list = [Data(x = features_t[idx], edge_index = edge_indices[idx],\n",
    "                       y = labels_t[idx]).to(device) for idx in train_idx ]\n",
    "    val_list = [Data(x = features_t[idx], edge_index = edge_indices[idx],\n",
    "                     y = labels_t[idx]).to(device) for idx in val_idx ]\n",
    "    test_list = [Data(x = features_t[idx], edge_index = edge_indices[idx],\n",
    "                      y = labels_t[idx]).to(device) for idx in test_idx ]\n",
    "    data = {}\n",
    "    data['train'] = train_list\n",
    "    data['val'] = val_list\n",
    "    data['test'] = test_list\n",
    "\n",
    "    return data, graph_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_helper(new_nodes, new_edges, labels):\n",
    "    \"\"\"\n",
    "    Split the graph and store node features, edges (represented by adjacency list),\n",
    "    and labels separately by timestamp t (from 1 to 49).\n",
    "\n",
    "    Args:\n",
    "        new_nodes     A dataframe of the node features\n",
    "        new_edges     A dataframe of the graph's adjacency list\n",
    "\n",
    "    Returns:\n",
    "        features_t    A list of (|N_t|, d) feature matrices by timestamp\n",
    "        edge_indices  A list of (2, |E_t|) adjacency list by timestamp\n",
    "        labels_t      A list of (|N_t|) labels by timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    features =  torch.FloatTensor(new_nodes.iloc[:, 2:].to_numpy())\n",
    "    times = new_nodes.iloc[:, 1].to_numpy()\n",
    "    times = torch.LongTensor(times.reshape(len(times),))\n",
    "    labels = labels.iloc[:, 1].to_numpy().astype(int)\n",
    "    labels = torch.LongTensor(labels.reshape(len(labels),))\n",
    "    labels -= 1\n",
    "\n",
    "    nodes_id = new_nodes.iloc[:, 0].to_numpy()\n",
    "    nodes_id = torch.LongTensor(nodes_id.reshape(len(nodes_id),))\n",
    "\n",
    "    # Construct nodes of the directed graph for each time step;\n",
    "    # features by time step are stored in \"features_t\"; labels by\n",
    "    # time step are stored in \"labels_t\"\n",
    "    features_t = features\n",
    "    labels_t = labels\n",
    "\n",
    "    # Create a dictionary where\n",
    "    # <key, value> = <node_id, <<idx, node_index_in_time_t_subgraph>, <t, time_t>>>.\n",
    "    id2idx = {}\n",
    "    nodes_t = nodes_id\n",
    "\n",
    "    for i in range(nodes_t.shape[0]):\n",
    "        id2idx[nodes_t[i].item()] = {}\n",
    "        id2idx[nodes_t[i].item()]['idx'] = i\n",
    "\n",
    "    # Construct adjacency lists of the directed graph (non-symmetric) for each time step;\n",
    "    # adjacency lists for each time step are stored in \"edge_indices\".\n",
    "    edge_idx_t = []\n",
    "    for index in range(new_edges.shape[0]):\n",
    "        node1_idx = id2idx[new_edges.iloc[index, 0]]['idx']\n",
    "        node2_idx = id2idx[new_edges.iloc[index, 1]]['idx']\n",
    "        edge_idx_t.append([node1_idx, node2_idx]) # time_step starts from 1\n",
    "\n",
    "    edge_indices = torch.LongTensor(edge_idx_t).t()\n",
    "\n",
    "    return features_t, edge_indices, labels_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T14:35:54.465656602Z",
     "start_time": "2023-06-10T14:35:54.461835904Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_graph(nodes, edges, labels, device='cpu'):\n",
    "    \"\"\"\n",
    "    Create and return the big graph,\n",
    "    Args:\n",
    "        nodes     A dataframe of the node features\n",
    "        edges     A dataframe of the graph's adjacency list\n",
    "        device        Computing device\n",
    "\n",
    "    Returns:\n",
    "        data          A dictionary that stores training, validation, and test set,\n",
    "                        each value is a list of Data object\n",
    "        graph_info    A matrix where each row contains information of the time-step subgraph\n",
    "                      [time_step, num_of_nodes, num_of_edges, num_of_illicit_nodes]\n",
    "    \"\"\"\n",
    "    features_t, edge_indices, labels_t = create_graph_helper(nodes, edges, labels)\n",
    "    graph_info = np.zeros((len(labels_t), 4), dtype = np.int64)\n",
    "    data = Data(x = features_t, edge_index = edge_indices, y = labels_t).to(device)\n",
    "\n",
    "    return data, graph_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T11:53:24.029009200Z",
     "start_time": "2023-05-28T11:53:24.029009200Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data, device, save_model_results=False):\n",
    "    \"\"\"\n",
    "    Test the model by using the given split datasets.\n",
    "\n",
    "    Args:\n",
    "        model                 The GCN model\n",
    "        data                  A dictionary of Data objects that store x, edge_index, and labels\n",
    "                                for three sets\n",
    "        save_model_results    A boolean determining whether we save the model results\n",
    "\n",
    "    Returns\n",
    "        The accuracy and auc-roc score of training, validation, and test set\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    # The output of model on each data sets\n",
    "    eval = {}\n",
    "    for name in data.keys():\n",
    "        data_list = data[name]\n",
    "        eval_report = []\n",
    "        eval_auc_roc = 0\n",
    "        for i, batch in enumerate(data_list):\n",
    "            out = model.forward(batch)\n",
    "            y_true = batch.y\n",
    "            indexes = y_true != 2\n",
    "\n",
    "            y_pred = out.argmax(dim=-1, keepdim=True)[indexes].view(-1, 1).cpu().detach().numpy()\n",
    "            y_true = y_true[indexes].view(-1, 1).cpu().detach().numpy()\n",
    "\n",
    "            acc = classification_report(y_true, y_pred,output_dict=True, zero_division=0)\n",
    "            eval_report.append(acc)\n",
    "            auc_roc = roc_auc_score(y_true, y_pred)\n",
    "            eval_auc_roc += auc_roc\n",
    "        report = {}\n",
    "        for key in eval_report[0].keys():\n",
    "            if type(eval_report[0][key]) is dict:\n",
    "                df = pd.DataFrame([sub_report[key] for sub_report in eval_report])\n",
    "                report[key] = df.mean().to_dict()\n",
    "            else:\n",
    "                report[key] = np.mean(np.array([sub_report[key] for sub_report in eval_report]))\n",
    "        eval_auc_roc /= len(data_list)\n",
    "        eval[name] = {'report': pd.DataFrame(report), 'auc_roc': eval_auc_roc}\n",
    "\n",
    "    ### TODO: what is the criterion to save the model results, the whole prediction\n",
    "    ### y_pred and y_true? or only the test sets' prediction?\n",
    "    if save_model_results:\n",
    "        print (\"Saving Model Predictions\")\n",
    "\n",
    "        data_new = {}\n",
    "        data_new ['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
    "\n",
    "        df = pd.DataFrame(data=data_new )\n",
    "        # Save locally as csv\n",
    "        df.to_csv('gcn_ind.csv', sep=',', index=False)\n",
    "\n",
    "    return eval['train']['report'], eval['val']['report'], eval['test']['report'], \\\n",
    "        eval['train']['auc_roc'], eval['val']['auc_roc'], eval['test']['auc_roc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T10:21:53.457433100Z",
     "start_time": "2023-06-24T10:21:53.441421700Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_data, optimizer, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Train the model by using the given optimizer and loss_fn.\n",
    "\n",
    "    Args:\n",
    "        model       The GCN model\n",
    "        train_data  The Data object that stores x, edge_index, and labels\n",
    "                      only for training set\n",
    "        optimizer   The optimizer\n",
    "        loss_fn     The loss function\n",
    "\n",
    "    Returns\n",
    "        The average prediction loss of each time step in the training set\n",
    "          by the given loss function\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss = torch.FloatTensor([0]*len(train_data)).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        batch = batch.to(device)\n",
    "        indexes = batch.y != 2\n",
    "\n",
    "        y_pred = model.forward(batch)[indexes, :]\n",
    "        y_true = batch.y[indexes]\n",
    "        loss[i] = loss_fn(y_pred, y_true)\n",
    "    loss.mean().backward()\n",
    "    optimizer.step()\n",
    "    return loss.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T14:28:52.563932337Z",
     "start_time": "2023-06-10T14:28:52.561141110Z"
    }
   },
   "outputs": [],
   "source": [
    "def isConnected(edge_index):\n",
    "    \"\"\"\n",
    "    Computes whether a graph is connected or not, based on its edge index\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # example of how to thing was implemented before and it worked\n",
    "    # adj = to_dense_adj(data['train'][0].edge_index).squeeze(0)\n",
    "    adj = to_dense_adj(edge_index).squeeze(0)\n",
    "    g = graphs.Graph(adj)\n",
    "    return g.is_connected()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
