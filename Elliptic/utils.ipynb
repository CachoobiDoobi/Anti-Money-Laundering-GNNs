{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T11:41:11.065486Z",
     "start_time": "2023-05-28T11:41:09.009571900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import pandas as pd\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from pygsp import graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T13:27:11.124672300Z",
     "start_time": "2023-05-28T13:27:11.109020200Z"
    }
   },
   "outputs": [],
   "source": [
    "def time_step_split_helper(new_nodes, new_edges, labels):\n",
    "    \"\"\"\n",
    "    Split the graph and store node features, edges (represented by adjacency list),\n",
    "    and labels separately by timestamp t (from 1 to 49).\n",
    "\n",
    "    Args:\n",
    "        new_nodes     A dataframe of the node features\n",
    "        new_edges     A dataframe of the graph's adjacency list\n",
    "\n",
    "    Returns:\n",
    "        features_t    A list of (|N_t|, d) feature matrices by timestamp\n",
    "        edge_indices  A list of (2, |E_t|) adjacency list by timestamp\n",
    "        labels_t      A list of (|N_t|) labels by timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    features =  torch.FloatTensor(new_nodes.iloc[:, 2:].to_numpy())\n",
    "    times = new_nodes.iloc[:, 1].to_numpy()\n",
    "    times = torch.LongTensor(times.reshape(len(times),))\n",
    "    labels = labels.iloc[:, 1].to_numpy().astype(int)\n",
    "    labels = torch.LongTensor(labels.reshape(len(labels),))\n",
    "    labels -= 1\n",
    "\n",
    "    nodes_id = new_nodes.iloc[:, 0].to_numpy()\n",
    "    nodes_id = torch.LongTensor(nodes_id.reshape(len(nodes_id),))\n",
    "\n",
    "    min_t = torch.min(times) # 1\n",
    "    max_t = torch.max(times) # 49\n",
    "\n",
    "    # Construct nodes of the directed graph for each time step;\n",
    "    # features by time step are stored in \"features_t\"; labels by\n",
    "    # time step are stored in \"labels_t\"\n",
    "    features_t = []\n",
    "    labels_t = []\n",
    "\n",
    "    # Create a dictionary where\n",
    "    # <key, value> = <node_id, <<idx, node_index_in_time_t_subgraph>, <t, time_t>>>.\n",
    "    id2idx = {}\n",
    "    for t in range(min_t, max_t + 1):\n",
    "        features_t.append(features[times == t, :])\n",
    "        labels_t.append(labels[times == t])\n",
    "        nodes_t = nodes_id[times == t]\n",
    "        for i in range(nodes_t.shape[0]):\n",
    "            id2idx[nodes_t[i].item()] = {}\n",
    "            id2idx[nodes_t[i].item()]['idx'] = i\n",
    "            id2idx[nodes_t[i].item()]['t'] = t\n",
    "\n",
    "    # Construct adjacency lists of the directed graph (non-symmetric) for each time step;\n",
    "    # adjacency lists for each time step are stored in \"edge_indices\".\n",
    "    edge_idx_t = [[] for _ in range(min_t, max_t + 1)]\n",
    "    for index in range(new_edges.shape[0]):\n",
    "        node1_t = id2idx[new_edges.iloc[index, 0]]['t']\n",
    "        node1_idx = id2idx[new_edges.iloc[index, 0]]['idx']\n",
    "        node2_t = id2idx[new_edges.iloc[index, 1]]['t']\n",
    "        node2_idx = id2idx[new_edges.iloc[index, 1]]['idx']\n",
    "        edge_idx_t[node1_t - 1].append([node1_idx, node2_idx]) # time_step starts from 1\n",
    "\n",
    "    edge_indices = [torch.LongTensor(edge_idx_t[i]).t() for i in range(len(edge_idx_t))]\n",
    "    return features_t, edge_indices, labels_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T11:41:11.112241800Z",
     "start_time": "2023-05-28T11:41:11.096620100Z"
    }
   },
   "outputs": [],
   "source": [
    "def time_step_split(new_nodes, new_edges, labels, device, train_lt = 31, val_lt = 36, test_lt = 49):\n",
    "    \"\"\"\n",
    "    Create and return the training, validation, and test set, splitted by time step,\n",
    "    where each subgraph at time t is considered as an input of GCN model.\n",
    "\n",
    "    Args:\n",
    "        new_nodes     A dataframe of the node features\n",
    "        new_edges     A dataframe of the graph's adjacency list\n",
    "        device        Computing device\n",
    "        train_lt      The last time step index of training set\n",
    "        val_lt        The last time step index of validation set\n",
    "        test_lt       The last time step index of test set\n",
    "\n",
    "    Returns:\n",
    "        data          A dictionary that stores training, validation, and test set,\n",
    "                        each value is a list of Data object\n",
    "        graph_info    A matrix where each row contains information of the time-step subgraph\n",
    "                      [time_step, num_of_nodes, num_of_edges, num_of_illicit_nodes]\n",
    "    \"\"\"\n",
    "    features_t, edge_indices, labels_t = time_step_split_helper(new_nodes, new_edges, labels)\n",
    "\n",
    "    graph_info = np.zeros((len(labels_t), 4), dtype = np.int64)\n",
    "    # for t in range(len(labels_t)):\n",
    "    #     if(edge_indices[t].shape != 0):\n",
    "    #         break\n",
    "    #     graph_info[t, :] = np.array([t, features_t[t].shape[0], edge_indices[t].shape[1],\n",
    "    #                                  labels_t[t][labels_t[t] == 1].shape[0]])\n",
    "\n",
    "    train_idx, val_idx, test_idx = [np.arange(train_lt), np.arange(train_lt, val_lt),\n",
    "                                    np.arange(val_lt, test_lt)]\n",
    "    train_list = [Data(x = features_t[idx], edge_index = edge_indices[idx],\n",
    "                       y = labels_t[idx]).to(device) for idx in train_idx ]\n",
    "    val_list = [Data(x = features_t[idx], edge_index = edge_indices[idx],\n",
    "                     y = labels_t[idx]).to(device) for idx in val_idx ]\n",
    "    test_list = [Data(x = features_t[idx], edge_index = edge_indices[idx],\n",
    "                      y = labels_t[idx]).to(device) for idx in test_idx ]\n",
    "    data = {}\n",
    "    data['train'] = train_list\n",
    "    data['val'] = val_list\n",
    "    data['test'] = test_list\n",
    "\n",
    "    return data, graph_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_helper(new_nodes, new_edges, labels):\n",
    "    \"\"\"\n",
    "    Split the graph and store node features, edges (represented by adjacency list),\n",
    "    and labels separately by timestamp t (from 1 to 49).\n",
    "\n",
    "    Args:\n",
    "        new_nodes     A dataframe of the node features\n",
    "        new_edges     A dataframe of the graph's adjacency list\n",
    "\n",
    "    Returns:\n",
    "        features_t    A list of (|N_t|, d) feature matrices by timestamp\n",
    "        edge_indices  A list of (2, |E_t|) adjacency list by timestamp\n",
    "        labels_t      A list of (|N_t|) labels by timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    features =  torch.FloatTensor(new_nodes.iloc[:, 2:].to_numpy())\n",
    "    times = new_nodes.iloc[:, 1].to_numpy()\n",
    "    times = torch.LongTensor(times.reshape(len(times),))\n",
    "    labels = labels.iloc[:, 1].to_numpy().astype(int)\n",
    "    labels = torch.LongTensor(labels.reshape(len(labels),))\n",
    "    labels -= 1\n",
    "\n",
    "    nodes_id = new_nodes.iloc[:, 0].to_numpy()\n",
    "    nodes_id = torch.LongTensor(nodes_id.reshape(len(nodes_id),))\n",
    "\n",
    "    # Construct nodes of the directed graph for each time step;\n",
    "    # features by time step are stored in \"features_t\"; labels by\n",
    "    # time step are stored in \"labels_t\"\n",
    "    features_t = features\n",
    "    labels_t = labels\n",
    "\n",
    "    # Create a dictionary where\n",
    "    # <key, value> = <node_id, <<idx, node_index_in_time_t_subgraph>, <t, time_t>>>.\n",
    "    id2idx = {}\n",
    "    nodes_t = nodes_id\n",
    "\n",
    "    for i in range(nodes_t.shape[0]):\n",
    "        id2idx[nodes_t[i].item()] = {}\n",
    "        id2idx[nodes_t[i].item()]['idx'] = i\n",
    "\n",
    "    # Construct adjacency lists of the directed graph (non-symmetric) for each time step;\n",
    "    # adjacency lists for each time step are stored in \"edge_indices\".\n",
    "    edge_idx_t = []\n",
    "    for index in range(new_edges.shape[0]):\n",
    "        node1_idx = id2idx[new_edges.iloc[index, 0]]['idx']\n",
    "        node2_idx = id2idx[new_edges.iloc[index, 1]]['idx']\n",
    "        edge_idx_t.append([node1_idx, node2_idx]) # time_step starts from 1\n",
    "\n",
    "    edge_indices = torch.LongTensor(edge_idx_t).t()\n",
    "\n",
    "    return features_t, edge_indices, labels_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(nodes, edges, labels, device='cpu'):\n",
    "    \"\"\"\n",
    "    Create and return the big graph,\n",
    "    Args:\n",
    "        nodes     A dataframe of the node features\n",
    "        edges     A dataframe of the graph's adjacency list\n",
    "        device        Computing device\n",
    "\n",
    "    Returns:\n",
    "        data          A dictionary that stores training, validation, and test set,\n",
    "                        each value is a list of Data object\n",
    "        graph_info    A matrix where each row contains information of the time-step subgraph\n",
    "                      [time_step, num_of_nodes, num_of_edges, num_of_illicit_nodes]\n",
    "    \"\"\"\n",
    "    features_t, edge_indices, labels_t = create_graph_helper(nodes, edges, labels)\n",
    "    graph_info = np.zeros((len(labels_t), 4), dtype = np.int64)\n",
    "    data = Data(x = features_t, edge_index = edge_indices, y = labels_t).to(device)\n",
    "\n",
    "    return data, graph_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T12:05:44.414862400Z",
     "start_time": "2023-06-23T12:05:44.380877900Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39mno_grad()\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_ilicit_predictions\u001B[39m(model, loader, test_mask, device):\n\u001B[0;32m      3\u001B[0m     model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(loader):\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_ilicit_predictions(model, loader, test_mask, device):\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        y_pred = model(batch.x, batch.edge_index).argmax(dim=-1)[test_mask].cpu().detach().numpy()\n",
    "        y_true = batch.y[test_mask].cpu().detach().numpy()\n",
    "\n",
    "    illicit = np.where(y_true == 0)[0]\n",
    "\n",
    "    correct = np.where(y_pred[illicit] == y_true[illicit])\n",
    "    wrong = np.where(y_pred[illicit] != y_true[illicit])\n",
    "    return correct, wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T11:53:24.029009200Z",
     "start_time": "2023-05-28T11:53:24.029009200Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, loader,loss_fn, test_mask, device):\n",
    "    model.eval()\n",
    "    loss = []\n",
    "    res = {}\n",
    "    for i, batch in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        indexes = batch.y != 2\n",
    "\n",
    "        y_pred = model(batch.x, batch.edge_index)[indexes, :]\n",
    "        y_true = batch.y[indexes]\n",
    "        res = classification_report(torch.unsqueeze(y_true, -1).cpu(),\n",
    "                                    y_pred.argmax(dim=-1).cpu(),output_dict=True, zero_division=0)\n",
    "        loss.append(loss_fn(y_pred, y_true).item())\n",
    "    return res, np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T17:18:06.062368100Z",
     "start_time": "2023-06-19T17:18:06.055318500Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_data,train_mask, optimizer, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Train the model by using the given optimizer and loss_fn.\n",
    "\n",
    "    Args:\n",
    "        model       The GCN model\n",
    "        train_data  The Data object that stores x, edge_index, and labels\n",
    "                      only for training set\n",
    "        optimizer   The optimizer\n",
    "        loss_fn     The loss function\n",
    "\n",
    "    Returns\n",
    "        The average prediction loss of each time step in the training set\n",
    "          by the given loss function\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss = torch.FloatTensor([0]*len(train_data)).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        indexes = batch.y != 2\n",
    "\n",
    "        train_slice = model.forward(batch.x, batch.edge_index)[indexes, :]\n",
    "        train_label = batch.y[indexes]\n",
    "\n",
    "        loss[i] = loss_fn(train_slice, train_label)\n",
    "    loss.mean().backward()\n",
    "    optimizer.step()\n",
    "    return loss.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T14:28:52.563932337Z",
     "start_time": "2023-06-10T14:28:52.561141110Z"
    }
   },
   "outputs": [],
   "source": [
    "def isConnected(edge_index):\n",
    "    \"\"\"\n",
    "    Computes whether a graph is connected or not, based on its edge index\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # example of how to thing was implemented before and it worked\n",
    "    # adj = to_dense_adj(data['train'][0].edge_index).squeeze(0)\n",
    "    adj = to_dense_adj(edge_index).squeeze(0)\n",
    "    g = graphs.Graph(adj)\n",
    "    return g.is_connected()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
